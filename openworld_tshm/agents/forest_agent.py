from __future__ import annotations
import os, subprocess, json


def generate_narrative(summary_inputs: dict, use_llm: str = "auto") -> str:
    """
    Generate a narrative report using OpenAI Agents SDK if OPENAI_API_KEY set,
    else try 'llm' CLI with ollama model if installed, else return a deterministic fallback.
    use_llm: "auto" | "openai" | "local" | "fallback"
    """
    prompt = (
        "You are a forest management assistant. Create a concise narrative summarizing "
        "tree species distribution, height statistics, and health, and recommend harvest timelines, "
        "planting densities, and regeneration targets.\n"
        f"Inputs JSON:\n{json.dumps(summary_inputs, indent=2)}\n"
        "Return 3-5 paragraphs."
    )

    if use_llm in ("openai", "auto") and os.getenv("OPENAI_API_KEY"):
        try:  # pragma: no cover - networked branch
            from openai import OpenAI  # type: ignore  # pragma: no cover
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))  # pragma: no cover
            resp = client.chat.completions.create(  # pragma: no cover
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),  # pragma: no cover
                messages=[{"role": "system", "content": "You are a forestry planning expert."},  # pragma: no cover
                          {"role": "user", "content": prompt}],  # pragma: no cover
                temperature=0.2,  # pragma: no cover
            )
            return resp.choices[0].message.content or ""  # pragma: no cover
        except Exception:
            # fall through to other modes or deterministic fallback
            pass  # pragma: no cover

    if use_llm in ("local", "auto") and os.getenv("OLLAMA_MODEL"):
        try:  # pragma: no cover - external binary branch
            model = os.getenv("OLLAMA_MODEL", "llama3")  # pragma: no cover
            text = subprocess.check_output(["llm", "-m", f"ollama:{model}", prompt], text=True, timeout=3)  # pragma: no cover
            return text.strip()  # pragma: no cover
        except Exception:
            pass  # pragma: no cover

    # Deterministic fallback
    trees = summary_inputs.get("num_trees", 0)
    avg_h = summary_inputs.get("avg_height", 0)
    return (
        f"This stand contains approximately {trees} trees with an average height of {avg_h:.1f} m. "
        "Health indicators suggest stable vigor with localized variability.\n"
        "We recommend staggered harvest windows aligned with species-specific rotation ages, "
        "and replanting at densities suitable for site class II. Regeneration targets should aim "
        "for 85â€“90% survival at year 3 with supplemental fill-in planting as needed."
    )

